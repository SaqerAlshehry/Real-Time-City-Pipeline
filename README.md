# ğŸ™ï¸ Smart City Real-Time Data Pipeline

A complete **real-time data engineering pipeline** that simulates smart city data and processes it using modern streaming technologies.

**Kafka â†’ Spark Structured Streaming â†’ PostgreSQL â†’ Streamlit Dashboard**

---

## ğŸ“Œ Overview

This project demonstrates how real-time data flows through a distributed system:
- Data is **produced continuously** to Kafka topics
- **Spark Structured Streaming** consumes and processes the data
- Processed data is stored in **PostgreSQL**
- A **Streamlit dashboard** visualizes live analytics

Everything runs locally using **Docker Compose**.

---

## ğŸ§± Architecture

Kafka Producer

â¬‡ï¸ 

Kafka Topics 

â¬‡ï¸

Spark Structured Streaming


â¬‡ï¸

PostgreSQL

â¬‡ï¸

Streamlit


â¬‡ï¸

Dashboard


---

## âš™ï¸ Prerequisites

### Install Docker Desktop
Download and install Docker Desktop:
- https://www.docker.com/products/docker-desktop

Verify installation:
```bash
docker --version
docker compose version
```

--- 
## â–¶ï¸ How to Run the Application
### 1ï¸âƒ£ Clone the Repo
```bash
git clone https://github.com/SaqerAlshehry/Real-Time-City-Pipeline.git
cd Real-Time-Smart-City-Pipeline
```
### 2ï¸âƒ£ Stop Containers & remove old data
```bash
docker compose down
docker volume rm real-time-smart-city-data-pipeline-with-kafka-spark-streaming-and-aws_postgres-data
```
### 3ï¸âƒ£ Start all services
```bash
docker compose up -d
```
### 4ï¸âƒ£ Wait for services to initialize
```bash
sleep 20
```
### 5ï¸âƒ£ Start the Spark streaming job
```bash
docker exec -it spark-master \
/opt/spark/bin/spark-submit \
--master spark://spark-master:7077 \
--conf spark.jars.ivy=/tmp/.ivy2 \
--packages \
org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,org.apache.kafka:kafka-clients:3.4.1,org.postgresql:postgresql:42.7.3 \
/opt/spark/jobs/spark-city.py
```

### 6ï¸âƒ£ Open the dashboard
```bash
http://localhost:8501
```
You should see live data updating in real time!

#### â˜‘ï¸ To stop the Application
```bash
docker compose down
```
